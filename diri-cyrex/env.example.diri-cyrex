# ========================================
# DEEPIRI PYTHON AGENT ENVIRONMENT
# ========================================
# Copy this to .env and configure your values
# Supports both local development and Docker deployments
#
# IMPORTANT: For Kubernetes deployments, use ConfigMaps/Secrets instead of .env files.
# This file is for local dev/Docker Compose only.

# ========================================
# SERVER CONFIGURATION
# ========================================
PORT=8000
HOST=0.0.0.0
ENVIRONMENT=development
DEBUG=true

# ========================================
# AI CONFIGURATION
# ========================================
# AI Provider: openai | localai | deepinfra
# Set to 'localai' for local/free deployment (runs on localhost:8080)
# Set to 'openai' for cloud deployment (requires API key)
# Set to 'deepinfra' for alternative cloud provider
AI_PROVIDER=openai

# OpenAI Configuration (Cloud Path)
OPENAI_API_KEY=sk-your-openai-api-key-here
OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini
OPENAI_AGENT_MODEL=gpt-4o-mini
OPENAI_MAX_TOKENS=2000
OPENAI_TEMPERATURE=0.5
OPENAI_TOP_P=0.9

# LocalAI Configuration (Local/Free Path)
# LocalAI is a drop-in replacement for OpenAI that runs locally
# Install: docker run -ti --name local-ai -p 8080:8080 localai/localai:latest
# Or use: curl https://localai.io/install.sh | sh
# For GPU support: docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-12
LOCALAI_API_BASE=http://localhost:8080/v1
LOCALAI_MODEL=gpt-4o-mini
LOCALAI_API_KEY=                  # Optional: set if LocalAI requires API key
# For Docker internal network, use: http://localai:8080/v1

# DeepInfra Configuration (Alternative Cloud Path)
DEEPINFRA_API_KEY=your-deepinfra-api-key-here
DEEPINFRA_API_BASE=https://api.deepinfra.com/v1/openai
DEEPINFRA_MODEL=meta-llama/Llama-3.1-8B-Instruct

# Agent Configuration
AGENT_TEMPERATURE=0.5
AGENT_TOP_P=0.9
AGENT_MAX_MEMORY=10
AGENT_SESSION_TIMEOUT=1800
AGENT_MAX_CONVERSATION_LENGTH=50

# ========================================
# SECURITY & AUTHENTICATION
# ========================================
CYREX_API_KEY=change-me
NODE_BACKEND_URL=http://localhost:5000
ALLOWED_ORIGINS=http://localhost:5173,http://localhost:3000

# Development URLs (Local/Docker)
DEV_CLIENT_URL=http://localhost:5173
DEV_API_URL=http://localhost:5000/api
DEV_CYREX_URL=http://localhost:8000

# Production URLs (Kubernetes ConfigMap Reference Only)
# For cloud K8s, set these in ConfigMaps, not .env files
# PROD_CLIENT_URL=https://deepiri.com
# PROD_API_URL=https://api.deepiri.com
# PROD_CYREX_URL=https://agent.deepiri.com

# CORS Configuration
CORS_ORIGIN=http://localhost:5173
CORS_CREDENTIALS=true
CORS_METHODS=GET,POST,PUT,DELETE,OPTIONS
CORS_HEADERS=Content-Type,Authorization,X-API-Key

# ========================================
# BACKEND COMMUNICATION
# ========================================
# Local Development
NODE_BACKEND_URL=http://localhost:5000

# Docker Internal Network URLs
INTERNAL_BACKEND_URL=http://backend:5000
INTERNAL_LOCALAI_URL=http://localai:8080/v1

# Kubernetes Internal URLs (for K8s deployments)
# In Kubernetes, use service names: backend-service, localai-service, etc.
# INTERNAL_BACKEND_URL=http://backend-service:5000
# INTERNAL_LOCALAI_URL=http://localai-service:8080/v1

BACKEND_API_KEY=your-backend-api-key-here
BACKEND_TIMEOUT=30

# Health Check
HEALTH_CHECK_INTERVAL=30
HEALTH_CHECK_TIMEOUT=5

# ========================================
# LOGGING CONFIGURATION
# ========================================
LOG_LEVEL=INFO
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s
LOG_FILE=logs/cyrex.log
LOG_MAX_SIZE=10485760
LOG_BACKUP_COUNT=5

# Python Logging Levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
UVICORN_LOG_LEVEL=info
FASTAPI_LOG_LEVEL=info

# ========================================
# PERFORMANCE & LIMITS
# ========================================
MAX_CONCURRENT_REQUESTS=100
REQUEST_TIMEOUT=60
MAX_REQUEST_SIZE=10485760

# Rate Limiting
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_WINDOW=60

# Memory Management
MAX_MEMORY_USAGE=512
CLEANUP_INTERVAL=300

# ========================================
# AI MODEL CONFIGURATION
# ========================================
# Model Selection (used when AI_PROVIDER is set)
PRIMARY_MODEL=gpt-4o-mini
FALLBACK_MODEL=gpt-3.5-turbo
MODEL_SWITCHING_ENABLED=true

# LocalAI Model Selection (for local deployments)
# Available models: llama-3.2-1b-instruct, phi-2, gemma:2b, etc.
# See: https://models.localai.io or run: local-ai models list
LOCALAI_PRIMARY_MODEL=llama-3.2-1b-instruct:q4_k_m
LOCALAI_FALLBACK_MODEL=phi-2

# Context Management
MAX_CONTEXT_LENGTH=4000
CONTEXT_WINDOW_OVERLAP=200
CONTEXT_COMPRESSION_ENABLED=true

# Response Configuration
MAX_RESPONSE_LENGTH=1000
RESPONSE_TIMEOUT=30
STREAM_RESPONSES=true

# ========================================
# CACHE CONFIGURATION
# ========================================
CACHE_ENABLED=true
CACHE_TTL=3600
CACHE_MAX_SIZE=1000
CACHE_TYPE=memory

# Redis Cache (if enabled)
REDIS_CACHE_URL=redis://localhost:6379/1
REDIS_CACHE_PASSWORD=your-redis-password

# ========================================
# RAG / EMBEDDINGS CONFIGURATION
# ========================================
# Vector Store: chromadb | postgres | mongodb | memory
VECTOR_STORE=chromadb

# ChromaDB Configuration (Local/Free Path)
CHROMADB_PATH=./chroma_db
CHROMADB_COLLECTION=deepiri_embeddings
CHROMADB_PERSIST=true

# Embedding Model (for LocalAI)
EMBEDDING_MODEL=all-minilm-l6-v2
EMBEDDING_DIMENSIONS=384

# Postgres Vector Store (Cloud Path - requires pgvector extension)
POSTGRES_VECTOR_STORE_URL=postgresql://user:pass@localhost:5432/deepiri
POSTGRES_VECTOR_TABLE=embeddings

# MongoDB Vector Store (Cloud Path)
MONGODB_VECTOR_STORE_URL=mongodb://localhost:27017/deepiri
MONGODB_VECTOR_COLLECTION=embeddings

# ========================================
# SESSION MANAGEMENT
# ========================================
SESSION_STORAGE=memory
SESSION_CLEANUP_INTERVAL=300
MAX_SESSIONS=1000
SESSION_PERSISTENCE=false

# ========================================
# MONITORING & METRICS
# ========================================
METRICS_ENABLED=true
METRICS_PORT=9091
PROMETHEUS_METRICS=true

# Health Checks
HEALTH_CHECK_ENDPOINT=/health
READINESS_CHECK_ENDPOINT=/ready
LIVENESS_CHECK_ENDPOINT=/alive

# ========================================
# DEVELOPMENT SETTINGS
# ========================================
AUTO_RELOAD=true
DEBUG_MODE=true
VERBOSE_LOGGING=true

# Testing
TEST_MODE=false
MOCK_OPENAI=false
MOCK_RESPONSES=false

# ========================================
# SECURITY SETTINGS
# ========================================
# API Key Validation
REQUIRE_API_KEY=true
API_KEY_HEADER=X-API-Key

# Request Validation
VALIDATE_REQUESTS=true
SANITIZE_INPUTS=true
MAX_INPUT_LENGTH=5000

# ========================================
# ERROR HANDLING
# ========================================
ERROR_REPORTING=true
ERROR_TRACKING_SERVICE=sentry
SENTRY_DSN=your-sentry-dsn-here

# Retry Configuration
MAX_RETRIES=3
RETRY_DELAY=1
BACKOFF_MULTIPLIER=2

# ========================================
# ADVANCED FEATURES
# ========================================
# Conversation Features
MEMORY_ENABLED=true
CONTEXT_AWARENESS=true
PERSONALITY_ENABLED=true

# Integration Features
EXTERNAL_API_INTEGRATION=true
WEBHOOK_SUPPORT=true
CALLBACK_URLS=http://localhost:5000/callbacks/cyrex

# ========================================
# DEPLOYMENT CONFIGURATION
# ========================================
# Container Settings
WORKERS=1
WORKER_CLASS=uvicorn.workers.UvicornWorker
KEEPALIVE=2
MAX_WORKER_CONNECTIONS=1000

# Graceful Shutdown
GRACEFUL_TIMEOUT=30
SHUTDOWN_TIMEOUT=60
