name: deepiri-ai-team

# AI Team Docker Compose Configuration
# 
# This file matches the services referenced in:
# - team_dev_environments/ai-team/build.sh
# - team_dev_environments/ai-team/start.sh
# - team_dev_environments/ai-team/stop.sh
#
# Primary services: cyrex, ollama, api-gateway, engagement-service, challenge-service, external-bridge-service
# Dependencies: postgres, redis, influxdb, etcd, minio, milvus, auth-service, task-orchestrator,
#   platform-analytics-service, notification-service, realtime-gateway
#
# Note: Ollama (local LLM) is only included in the AI team environment
#
# IMPORTANT: Set DOCKER_BUILDKIT=1 before building to enable layer caching
# This prevents 50GB+ of dangling images on every build

x-build-args: &build-args
  # Use BuildKit for better caching and no dangling images
  DOCKER_BUILDKIT: 1
  BUILDKIT_PROGRESS: plain

# Minimal logging - logs are automatically cleared and not saved
x-logging: &minimal-logging
  driver: "local"
  options:
    max-size: "1m"      # Only 1MB per file
    max-file: "1"       # Only 1 file (no rotation backup)
    compress: "false"   # No compression (faster cleanup)

services:
  # PostgreSQL Database - Primary database for users, roles, tasks, quests, metadata
  postgres:
    image: postgres:16-alpine
    container_name: deepiri-postgres-ai
    restart: unless-stopped
    logging: *minimal-logging
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-deepiri}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-deepiripassword}
      POSTGRES_DB: ${POSTGRES_DB:-deepiri}
    volumes:
      - postgres_ai_data:/var/lib/postgresql/data
      - ./scripts/postgres-init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    networks:
      - deepiri-ai-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-deepiri}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # pgAdmin - Database Admin UI
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: deepiri-pgadmin-ai
    restart: unless-stopped
    logging: *minimal-logging
    ports:
      - "5050:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL:-admin@deepiri.com}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD:-admin}
      PGADMIN_CONFIG_SERVER_MODE: 'False'
    volumes:
      - pgadmin_ai_data:/var/lib/pgadmin
    depends_on:
      - postgres
    networks:
      - deepiri-ai-network

  # Adminer - Lightweight Database Viewer (for quick inspection)
  adminer:
    image: adminer:latest
    container_name: deepiri-adminer-ai
    restart: unless-stopped
    logging: *minimal-logging
    ports:
      - "8080:8080"
    environment:
      ADMINER_DEFAULT_SERVER: postgres
    depends_on:
      - postgres
    networks:
      - deepiri-ai-network

  # Redis Cache
  redis:
    image: redis:7.2-alpine
    container_name: deepiri-redis-ai
    restart: unless-stopped
    logging: *minimal-logging
    ports:
      - "6380:6379"
    command: redis-server --requirepass ${REDIS_PASSWORD:-redispassword} --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_ai_data:/data
    networks:
      - deepiri-ai-network

  # InfluxDB for Time-Series Analytics
  influxdb:
    image: influxdb:2.7
    container_name: deepiri-influxdb-ai
    restart: unless-stopped
    logging: *minimal-logging
    ports:
      - "8086:8086"
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME: ${INFLUXDB_USER:-admin}
      DOCKER_INFLUXDB_INIT_PASSWORD: ${INFLUXDB_PASSWORD:-adminpassword}
      DOCKER_INFLUXDB_INIT_ORG: ${INFLUXDB_ORG:-deepiri}
      DOCKER_INFLUXDB_INIT_BUCKET: ${INFLUXDB_BUCKET:-analytics}
      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: ${INFLUXDB_TOKEN:-your-influxdb-token}
    volumes:
      - influxdb_ai_data:/var/lib/influxdb2
    networks:
      - deepiri-ai-network

  # etcd service for Milvus metadata
  etcd:
    image: quay.io/coreos/etcd:v3.5.5
    container_name: deepiri-etcd-ai
    restart: unless-stopped
    logging: *minimal-logging
    environment:
      ETCD_AUTO_COMPACTION_MODE: revision
      ETCD_AUTO_COMPACTION_RETENTION: 1000
      ETCD_QUOTA_BACKEND_BYTES: 4294967296
      ETCD_SNAPSHOT_COUNT: 50000
    volumes:
      - etcd_ai_data:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    networks:
      - deepiri-ai-network

  # MinIO service for Milvus object storage
  # Updated to latest version (2024) with proper parity settings for data redundancy
  minio:
    image: minio/minio:latest
    container_name: deepiri-minio-ai
    restart: unless-stopped
    logging: *minimal-logging
    environment:
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minioadmin}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY:-minioadmin}
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
    volumes:
      - minio_ai_data:/data
    # command: minio server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 40s
    ports:
      - "9000:9000"  # API
      - "9001:9001"  # Console
    command: minio server /data --console-address ":9001"
    networks:
      - deepiri-ai-network

  # Milvus Vector Database for RAG
  milvus:
    image: milvusdb/milvus:v2.3.0
    container_name: deepiri-milvus-ai
    restart: unless-stopped
    logging: *minimal-logging
    command: ["milvus", "run", "standalone"]
    ports:
      - "19530:19530"
      - "9091:9091"
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
    volumes:
      - milvus_ai_data:/var/lib/milvus
    depends_on:
      - etcd
      - minio
    networks:
      - deepiri-ai-network

  # Ollama (Local LLM Service)
  ollama:
    image: ollama/ollama:latest
    container_name: deepiri-ollama-ai
    restart: unless-stopped
    logging: *minimal-logging
    ports:
      - "11434:11434"
    volumes:
      - ollama_ai_data:/root/.ollama
    networks:
      - deepiri-ai-network
    # Optional: Uncomment for GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Python Agent API (AI Service)
  cyrex:
    build:
      context: ./diri-cyrex
      dockerfile: Dockerfile
      args:
        BUILD_TYPE: prebuilt
        BASE_IMAGE: python:3.11-slim
        PYTORCH_VERSION: 2.9.1
        PYTHON_VERSION: 3.11
    image: deepiri-dev-cyrex:latest
    pull_policy: never
    container_name: deepiri-cyrex-ai
    restart: unless-stopped
    logging: *minimal-logging
    environment:
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_MODEL: ${OPENAI_MODEL:-gpt-4o-mini}
      CORS_ORIGIN: http://localhost:5173
      CYREX_API_KEY: ${CYREX_API_KEY:-change-me}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-http://mlflow:5000}
      WANDB_API_KEY: ${WANDB_API_KEY}
      MODEL_REGISTRY_PATH: /app/model_registry
      STAGING_MODEL_PATH: /app/models/staging
      PRODUCTION_MODEL_PATH: /app/models/production
      PINECONE_API_KEY: ${PINECONE_API_KEY}
      PINECONE_ENVIRONMENT: ${PINECONE_ENVIRONMENT:-us-east1-gcp}
      PINECONE_INDEX: ${PINECONE_INDEX:-deepiri}
      WEAVIATE_URL: ${WEAVIATE_URL}
      INFLUXDB_URL: http://influxdb:8086
      INFLUXDB_TOKEN: ${INFLUXDB_TOKEN}
      INFLUXDB_ORG: ${INFLUXDB_ORG:-deepiri}
      INFLUXDB_BUCKET: ${INFLUXDB_BUCKET:-analytics}
      MILVUS_HOST: milvus
      MILVUS_PORT: 19530
      # Local LLM (Ollama) Configuration
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://ollama:11434}
      LOCAL_LLM_BACKEND: ${LOCAL_LLM_BACKEND:-ollama}
      LOCAL_LLM_MODEL: ${LOCAL_LLM_MODEL:-llama3:8b}
      TRANSFORMERS_CACHE: /app/.cache/huggingface
      HF_HOME: /app/.cache/huggingface
      SENTENCE_TRANSFORMERS_HOME: /app/.cache/sentence_transformers
    ports:
      - "8000:8000"
    volumes:
      - ./diri-cyrex/app:/app/app
      - ./diri-cyrex/train:/app/train
      - ./diri-cyrex/inference:/app/inference
      - cyrex_ai_cache:/app/.cache
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload --reload-dir /app/app
    depends_on:
      - influxdb
      - milvus
      - ollama
    networks:
      - deepiri-ai-network

  # Cyrex Interface (Frontend)
  cyrex-interface:
    image: node:20-alpine
    container_name: deepiri-cyrex-interface-ai
    restart: unless-stopped
    logging: *minimal-logging
    working_dir: /app
    environment:
      VITE_CYREX_BASE_URL: http://cyrex:8000
      VITE_PORT: 5175
    ports:
      - "5175:5175"
    volumes:
      - ./diri-cyrex/cyrex-interface:/app
      - /app/node_modules
    command: sh -c "npm install --legacy-peer-deps && npm run dev -- --host 0.0.0.0 --port ${VITE_PORT:-5175}"
    depends_on:
      - cyrex
    networks:
      - deepiri-ai-network

  # MLflow for AI Experiment Tracking
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.8.1
    container_name: deepiri-mlflow-ai
    restart: unless-stopped
    logging: *minimal-logging
    ports:
      - "5500:5000"
    environment:
      BACKEND_STORE_URI: file:/mlflow
      DEFAULT_ARTIFACT_ROOT: file:/mlflow/artifacts
    command: mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri file:/mlflow --default-artifact-root file:/mlflow/artifacts
    volumes:
      - mlflow_ai_data:/mlflow
    networks:
      - deepiri-ai-network

  # Jupyter Notebook for AI Research
  jupyter:
    build:
      context: ./diri-cyrex
      dockerfile: Dockerfile.jupyter
      args:
        BUILD_TYPE: prebuilt
        BASE_IMAGE: python:3.11-slim
        PYTORCH_VERSION: 2.9.1
        PYTHON_VERSION: 3.11
    image: deepiri-dev-jupyter:latest
    pull_policy: never
    container_name: deepiri-jupyter-ai
    restart: unless-stopped
    logging: *minimal-logging
    command: jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token='' --NotebookApp.password='' --NotebookApp.runtime_dir=/tmp/jupyter_runtime --NotebookApp.notebook_dir=/app/notebooks
    ports:
      - "8888:8888"
    volumes:
      - ./diri-cyrex/train/notebooks:/app/notebooks
      - ./diri-cyrex/train/data:/app/data
    environment:
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      JUPYTER_RUNTIME_DIR: /tmp/jupyter_runtime
    networks:
      - deepiri-ai-network

  # Challenge Service
  challenge-service:
    build:
      context: ./platform-services
      dockerfile: backend/deepiri-challenge-service/Dockerfile
    image: deepiri-dev-challenge-service:latest
    pull_policy: never
    container_name: deepiri-challenge-service-ai
    restart: unless-stopped
    logging: *minimal-logging
    ports:
      - "5007:5007"
    environment:
      NODE_ENV: development
      PORT: 5007
      DATABASE_URL: postgresql://${POSTGRES_USER:-deepiri}:${POSTGRES_PASSWORD:-deepiripassword}@postgres:5432/${POSTGRES_DB:-deepiri}
      CYREX_URL: http://cyrex:8000
    volumes:
      - ./platform-services/backend/deepiri-challenge-service:/app
      - ./platform-services/shared/deepiri-shared-utils:/shared-utils
      - /app/node_modules
      - /shared-utils/node_modules
    command: sh -c "cd /shared-utils && rm -rf node_modules/.caniuse-lite* 2>/dev/null || true && npm cache clean --force && npm install --legacy-peer-deps && npm run build && cd /app && npm cache clean --force && npm install --legacy-peer-deps file:/shared-utils && npm run dev"
    depends_on:
      - postgres
      - cyrex
    networks:
      - deepiri-ai-network

  # Engagement Service
  engagement-service:
    build:
      context: ./platform-services
      dockerfile: backend/deepiri-dev-engagement-service/Dockerfile
    image: deepiri-dev-engagement-service:latest
    pull_policy: never
    container_name: deepiri-engagement-service-ai
    restart: unless-stopped
    logging: *minimal-logging
    ports:
      - "5003:5003"
    # Variables loaded from ops/k8s/configmaps/engagement-service-configmap.yaml
    # Variables loaded from ops/k8s/secrets/secrets.yaml
    environment:
      # Override or add docker-specific variables here if needed
      PORT: 5003
      DATABASE_URL: postgresql://${POSTGRES_USER:-deepiri}:${POSTGRES_PASSWORD:-deepiripassword}@postgres:5432/${POSTGRES_DB:-deepiri}
    volumes:
      - ./platform-services/backend/deepiri-engagement-service:/app
      - ./platform-services/shared/deepiri-shared-utils:/shared-utils
      - shared_utils_node_modules:/shared-utils/node_modules
      - /app/node_modules
    command: sh -c "cd /shared-utils && if [ ! -d node_modules ] || [ -z \"$(ls -A node_modules 2>/dev/null)\" ]; then npm install --legacy-peer-deps --unsafe-perm && npm run build && chown -R nodejs:nodejs node_modules 2>/dev/null || true; fi && cd /app && rm -rf node_modules 2>/dev/null || true && npm cache clean --force && npm install --legacy-peer-deps file:/shared-utils && npm run dev"
    depends_on:
      - postgres
      - redis
    networks:
      - deepiri-ai-network

  # External Bridge Service
  external-bridge-service:
    build:
      context: ./platform-services
      dockerfile: backend/deepiri-external-bridge-service/Dockerfile
    image: deepiri-dev-external-bridge-service:latest
    pull_policy: never
    container_name: deepiri-engagement-service
    restart: unless-stopped
    logging: *minimal-logging
    ports:
      - "5006:5006"
    # Variables loaded from ops/k8s/configmaps/external-bridge-service-configmap.yaml
    # Variables loaded from ops/k8s/secrets/secrets.yaml
    environment:
      # Override or add docker-specific variables here if needed
      PORT: 5006
      DATABASE_URL: postgresql://${POSTGRES_USER:-deepiri}:${POSTGRES_PASSWORD:-deepiripassword}@postgres:5432/${POSTGRES_DB:-deepiri}
    volumes:
      - ./platform-services/backend/deepiri-external-bridge-service:/app
      - ./platform-services/shared/deepiri-shared-utils:/shared-utils
      - shared_utils_node_modules:/shared-utils/node_modules
      - /app/node_modules
    command: sh -c "cd /shared-utils && if [ ! -d node_modules ] || [ -z \"$(ls -A node_modules 2>/dev/null)\" ]; then npm install --legacy-peer-deps --unsafe-perm && npm run build && chown -R nodejs:nodejs node_modules 2>/dev/null || true; fi && cd /app && rm -rf node_modules 2>/dev/null || true && npm cache clean --force && npm install --legacy-peer-deps file:/shared-utils && npm run dev"
    depends_on:
      - postgres
    networks:
      - deepiri-ai-network

volumes:
  postgres_ai_data:
    driver: local
  pgadmin_ai_data:
    driver: local
  adminer_ai_data:
    driver: local
  redis_ai_data:
    driver: local
  mlflow_ai_data:
    driver: local
  influxdb_ai_data:
    driver: local
  milvus_ai_data:
    driver: local
  etcd_ai_data:
    driver: local
  minio_ai_data:
    driver: local
  cyrex_ai_cache:
    driver: local
  ollama_ai_data:
    driver: local
  shared_utils_node_modules:
    driver: local

networks:
  deepiri-ai-network:
    driver: bridge

